{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "250e4813",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5be15415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "from datetime import date\n",
    "import requests\n",
    "import time\n",
    "\n",
    "### For Google Sheets ###\n",
    "import os\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import gspread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9111e1",
   "metadata": {},
   "source": [
    "## Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6123e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find local path\n",
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8c5a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get a list of all CSV files in current directory\n",
    "filelist = glob.glob(os.path.join(path,'*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57472931",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read in old dataframe\n",
    "for f in filelist:\n",
    "    if re.search('ALL_DEEDS',f):\n",
    "        old_df = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfae607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove any CSV files in current directory\n",
    "for f in filelist:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcdb5bb",
   "metadata": {},
   "source": [
    "Web link: https://crs.cookcountyclerkil.gov/Search/Additional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31232b29",
   "metadata": {},
   "source": [
    "## Main Doc Scraper\n",
    "(Deeds & mortgages, those with a consideration amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c26d65f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set up start date, end date, and doc list\n",
    "\n",
    "start_date = '01012022'\n",
    "end_date = '03132023'\n",
    "\n",
    "run_date = date.today().strftime('%b-%d-%Y')\n",
    "\n",
    "doc_list = [\n",
    "    'DEED',\n",
    "    'DEIT',\n",
    "    'QCD',\n",
    "    'SPWD',\n",
    "    'TRUD',\n",
    "    'TEED',\n",
    "    'WARD',\n",
    "    'MORT'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb60e93",
   "metadata": {},
   "source": [
    "Target website: https://crs.cookcountyclerkil.gov/Search/Additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f6b7638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_scraper(page, page_counter):\n",
    "\n",
    "    dfs = pd.read_html(page)\n",
    "    df = dfs[0]\n",
    "\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    link_suffix = 'https://crs.cookcountyclerkil.gov/'\n",
    "\n",
    "    deed_urls = []\n",
    "    for link in soup.find_all('a',attrs={'href': re.compile('^/Document/Detail')}):\n",
    "        page = link_suffix + link.get('href')\n",
    "        deed_urls.append(page)\n",
    "\n",
    "    df['deed_urls'] = deed_urls\n",
    "\n",
    "    df['Consi. Amt.'] = df['Consi. Amt.'].str.replace('$','',regex=False)\n",
    "    df['Consi. Amt.'] = df['Consi. Amt.'].str.replace(',','',regex=False)\n",
    "\n",
    "    df['Consi. Amt.'] = pd.to_numeric(df['Consi. Amt.'])\n",
    "\n",
    "    page_counter = page_counter + 1\n",
    "\n",
    "    df.to_csv(f'{doc}_page{page_counter}_{start_date}_to_{end_date}.csv')\n",
    "    \n",
    "    return page_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afc502b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Run main doc scraper\n",
    "\n",
    "for doc in doc_list:\n",
    "    \n",
    "    page_counter = 0\n",
    "\n",
    "    playwright = await async_playwright().start()\n",
    "    browser = await playwright.chromium.launch(headless = True)\n",
    "    page = await browser.new_page()\n",
    "\n",
    "    # Go to https://crs.cookcountyclerkil.gov/Search/Additional\n",
    "    await page.goto(\"https://crs.cookcountyclerkil.gov/Search/Additional\")\n",
    "\n",
    "    # Click text=Document Type Search\n",
    "    await page.locator(\"text=Document Type Search\").click()\n",
    "\n",
    "    # Select DEED\n",
    "    await page.locator(\"text=Document Type * ABROGATION ACCEPTANCE ACCEPTANCE OF TRANFER ON DEATH INSTRUMEN A >> select[name=\\\"DocumentType\\\"]\").select_option(doc)\n",
    "\n",
    "    # Click text=From Date * (mm/dd/yyyy) >> input[name=\"RecordedFromDate\"]\n",
    "    await page.locator(\"text=From Date * (mm/dd/yyyy) >> input[name=\\\"RecordedFromDate\\\"]\").click()\n",
    "\n",
    "    await page.locator(\"text=From Date * (mm/dd/yyyy) >> input[name=\\\"RecordedFromDate\\\"]\").fill(start_date)\n",
    "\n",
    "    # Click text=To Date * (mm/dd/yyyy) >> input[name=\"RecordedToDate\"]\n",
    "    await page.locator(\"text=To Date * (mm/dd/yyyy) >> input[name=\\\"RecordedToDate\\\"]\").click()\n",
    "\n",
    "    await page.locator(\"text=To Date * (mm/dd/yyyy) >> input[name=\\\"RecordedToDate\\\"]\").fill(end_date)\n",
    "\n",
    "    # Click input[name=\"LowerLimit\"]\n",
    "    await page.locator(\"input[name=\\\"LowerLimit\\\"]\").click()\n",
    "\n",
    "    # Fill input[name=\"LowerLimit\"]\n",
    "    await page.locator(\"input[name=\\\"LowerLimit\\\"]\").fill(\"4000000\")\n",
    "\n",
    "    # Click text=Document Type Search Document Type * ABROGATION ACCEPTANCE ACCEPTANCE OF TRANFER >> button[name=\"submitButton\"]\n",
    "    await page.locator(\"text=Document Type Search Document Type * ABROGATION ACCEPTANCE ACCEPTANCE OF TRANFER >> button[name=\\\"submitButton\\\"]\").click()\n",
    "\n",
    "    try:\n",
    "        \n",
    "        await page.wait_for_selector(\"div[class=table-responsive]\")\n",
    "\n",
    "        x = await page.content()\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            page_scraper(x, page_counter)\n",
    "            \n",
    "        except Exception as e:\n",
    "            2 + 2\n",
    "            \n",
    "        i = 1\n",
    "        \n",
    "        while i < 100:\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                await page.wait_for_selector(\"div[class=table-responsive]\")\n",
    "\n",
    "                await page.locator(\"text=Â»\").click()\n",
    "                \n",
    "                y = await page.content()\n",
    "\n",
    "                page_scraper(y, i)\n",
    "                \n",
    "                i += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                \n",
    "                i += 1000\n",
    "                \n",
    "                await page.wait_for_selector(\"div[class=table-responsive]\")\n",
    "                \n",
    "                y = await page.content()\n",
    "\n",
    "                page_scraper(y, i)\n",
    "        \n",
    "        await browser.close()\n",
    "\n",
    "    # ---------------------\n",
    "#         await browser.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        2 + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77925e1",
   "metadata": {},
   "source": [
    "## Alternate Doc Scraper\n",
    "(Bankruptcy and Lis Pendens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1cfcb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_doc_list = [\n",
    "    'FORF',\n",
    "    'LISP',\n",
    "    'LISF'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fc891b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alt_page_scraper(page, page_counter):\n",
    "\n",
    "    dfs = pd.read_html(page)\n",
    "    df = dfs[0]\n",
    "\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    link_suffix = 'https://crs.cookcountyclerkil.gov/'\n",
    "\n",
    "    deed_urls = []\n",
    "    for link in soup.find_all('a',attrs={'href': re.compile('^/Document/Detail')}):\n",
    "        page = link_suffix + link.get('href')\n",
    "        deed_urls.append(page)\n",
    "\n",
    "    df['deed_urls'] = deed_urls\n",
    "\n",
    "    page_counter = page_counter + 1\n",
    "\n",
    "    df.to_csv(f'{doc}_page{page_counter}_{start_date}_to_{end_date}.csv')\n",
    "    \n",
    "    return page_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac9c32cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function failed: Can only use .str accessor with string values!\n",
      "Function failed: Can only use .str accessor with string values!\n",
      "Function failed: Length of values (183) does not match length of index (100)\n"
     ]
    }
   ],
   "source": [
    "### Run alt doc scraper\n",
    "\n",
    "for doc in alt_doc_list:\n",
    "    \n",
    "    page_counter = 0\n",
    "\n",
    "    playwright = await async_playwright().start()\n",
    "    browser = await playwright.chromium.launch(headless = True)\n",
    "    page = await browser.new_page()\n",
    "\n",
    "    # Go to https://crs.cookcountyclerkil.gov/Search/Additional\n",
    "    await page.goto(\"https://crs.cookcountyclerkil.gov/Search/Additional\")\n",
    "\n",
    "    # Click text=Document Type Search\n",
    "    await page.locator(\"text=Document Type Search\").click()\n",
    "\n",
    "    # Select DEED\n",
    "    await page.locator(\"text=Document Type * ABROGATION ACCEPTANCE ACCEPTANCE OF TRANFER ON DEATH INSTRUMEN A >> select[name=\\\"DocumentType\\\"]\").select_option(doc)\n",
    "\n",
    "    # Click text=From Date * (mm/dd/yyyy) >> input[name=\"RecordedFromDate\"]\n",
    "    await page.locator(\"text=From Date * (mm/dd/yyyy) >> input[name=\\\"RecordedFromDate\\\"]\").click()\n",
    "\n",
    "    await page.locator(\"text=From Date * (mm/dd/yyyy) >> input[name=\\\"RecordedFromDate\\\"]\").fill(start_date)\n",
    "\n",
    "    # Click text=To Date * (mm/dd/yyyy) >> input[name=\"RecordedToDate\"]\n",
    "    await page.locator(\"text=To Date * (mm/dd/yyyy) >> input[name=\\\"RecordedToDate\\\"]\").click()\n",
    "\n",
    "    await page.locator(\"text=To Date * (mm/dd/yyyy) >> input[name=\\\"RecordedToDate\\\"]\").fill(end_date)\n",
    "\n",
    "    # Click text=Document Type Search Document Type * ABROGATION ACCEPTANCE ACCEPTANCE OF TRANFER >> button[name=\"submitButton\"]\n",
    "    await page.locator(\"text=Document Type Search Document Type * ABROGATION ACCEPTANCE ACCEPTANCE OF TRANFER >> button[name=\\\"submitButton\\\"]\").click()\n",
    "\n",
    "    try:\n",
    "        \n",
    "        await page.wait_for_selector(\"div[class=table-responsive]\")\n",
    "\n",
    "        x = await page.content()\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            page_scraper(x, page_counter)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Function failed: {e}')\n",
    "            \n",
    "        i = 1\n",
    "        \n",
    "        while i < 100:\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                await page.wait_for_selector(\"div[class=table-responsive]\")\n",
    "\n",
    "                await page.locator(\"text=Â»\").click()\n",
    "                \n",
    "                y = await page.content()\n",
    "\n",
    "                page_scraper(y, i)\n",
    "                \n",
    "                i += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                \n",
    "                i += 1000\n",
    "                \n",
    "                await page.wait_for_selector(\"div[class=table-responsive]\")\n",
    "                \n",
    "                y = await page.content()\n",
    "\n",
    "                page_scraper(y, i)\n",
    "        \n",
    "        await browser.close()\n",
    "\n",
    "    # ---------------------\n",
    "#         await browser.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        2 + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca43c843",
   "metadata": {},
   "source": [
    "## Create Master CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "931a16c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Join all created CSVs into one file\n",
    "\n",
    "all_csvs = glob.glob(os.path.join(path,'*.csv'))\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_csvs:\n",
    "    frame = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(frame)\n",
    "    \n",
    "df = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b31f4362",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clean master CSV and edit data types\n",
    "\n",
    "df = df.drop(columns=['Unnamed: 0.1','Unnamed: 0','View Doc'])\n",
    "df['Consi. Amt.'] = df['Consi. Amt.'].apply(lambda x : '${:,}'.format(x))\n",
    "\n",
    "df['Doc Recorded'] = pd.to_datetime(df['Doc Recorded'])\n",
    "df = df.sort_values(by='Doc Recorded', ascending=False)\n",
    "\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dde281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', 500)\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "# pd.set_option('display.max_rows', 1600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d36ddab",
   "metadata": {},
   "source": [
    "## Get Links to docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d621651",
   "metadata": {},
   "outputs": [],
   "source": [
    "deed_list = df['deed_urls'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "975409d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###\n",
    "# x[1] = Grantor table\n",
    "# x[2] = Grantee table\n",
    "# x[3] = Legal Description and Subdivision table\n",
    "###\n",
    "\n",
    "grantor_list = []\n",
    "grantee_list = []\n",
    "PIN_list = []\n",
    "url_counter = 0\n",
    "\n",
    "for deed in deed_list:\n",
    "    page = requests.get(deed)\n",
    "    x = pd.read_html(page.content)\n",
    "    \n",
    "    try:\n",
    "        x_1 = x[1]\n",
    "        x_1 = x_1.drop('Trust#',axis=1)\n",
    "        x_1 = pd.DataFrame({'Name': [', '.join(x_1['Name'].str.strip('\"').tolist())]})\n",
    "        x_1 = x_1['Name'].to_list()\n",
    "        grantor_list.append(x_1)\n",
    "    except Exception as e:\n",
    "        2 + 2\n",
    "    \n",
    "    try:\n",
    "        x_2 = x[2]\n",
    "        x_2 = x_2.drop('Trust#',axis=1)\n",
    "        x_2 = pd.DataFrame({'Name': [', '.join(x_2['Name'].str.strip('\"').tolist())]})\n",
    "        x_2 = x_2['Name'].to_list()\n",
    "        grantee_list.append(x_2)\n",
    "    except Exception as e:\n",
    "        2 + 2\n",
    "    \n",
    "    try:\n",
    "        x_3 = x[3]\n",
    "        x_3 = x_3.filter(items=['Property Index # (PIN)'])\n",
    "        x_3 = pd.DataFrame({'PIN': [', '.join(x_3['Property Index # (PIN)'].str.strip('\"').tolist())]})\n",
    "        x_3 = x_3['PIN'].to_list()\n",
    "        PIN_list.append(x_3)\n",
    "    except Exception as e:\n",
    "        PIN_list.append('No PIN found')\n",
    "        \n",
    "    url_counter += 1\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8b263fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['grantor_all'] = grantor_list\n",
    "df['grantee_all'] = grantee_list\n",
    "df['PIN_all'] = PIN_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0512d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af2c23d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = f'ALL_DEEDS_{start_date}_to_{end_date}_run_{run_date}.csv'\n",
    "df.to_csv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68a63f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deed = df.loc[df['Doc Type'] != 'MORTGAGE'].reset_index()\n",
    "df_deed = df_deed.drop(columns=['level_0','index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea70892",
   "metadata": {},
   "source": [
    "## Google Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdf7bdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopes = [\n",
    "'https://www.googleapis.com/auth/spreadsheets',\n",
    "'https://www.googleapis.com/auth/drive'\n",
    "]\n",
    "\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_dict(\n",
    "    json.loads(os.environ.get('SERVICE_ACCOUNT_JSON')), scopes)\n",
    "file = gspread.authorize(credentials)\n",
    "sheet = file.open(\"CookCountyScraper\")\n",
    "sheet = sheet.sheet1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65acb218",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08d58863",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Unnamed: 0','index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30642096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rq/0_j_rksd7psgdycg9c1qwf6m0000gp/T/ipykernel_53968/593088941.py:1: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  df = df.replace([pd.np.inf, -pd.np.inf, pd.np.nan], 'NA')\n"
     ]
    }
   ],
   "source": [
    "df = df.replace([pd.np.inf, -pd.np.inf, pd.np.nan], 'NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1da93856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spreadsheetId': '1J-kWQ3SabAKz44NNYA3H7OE4NfMxdmk4AND2z_mh-AQ',\n",
       " 'clearedRange': 'Sheet1!A1:Z1498'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear existing data (optional)\n",
    "sheet.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "338e2680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spreadsheetId': '1J-kWQ3SabAKz44NNYA3H7OE4NfMxdmk4AND2z_mh-AQ',\n",
       " 'updates': {'spreadsheetId': '1J-kWQ3SabAKz44NNYA3H7OE4NfMxdmk4AND2z_mh-AQ',\n",
       "  'updatedRange': 'Sheet1!A2:M249',\n",
       "  'updatedRows': 248,\n",
       "  'updatedColumns': 13,\n",
       "  'updatedCells': 3224}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = df.columns.tolist()\n",
    "data = df.values.tolist()\n",
    "sheet.insert_row(header, 1)\n",
    "sheet.insert_rows(data, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
